#!/usr/bin/env python3
"""
Add New Site Scraper (LLM-Assisted with Comprehensive Analysis)
================================================================

Generates a new site scraper using powerful LLM to analyze URL patterns
and extract category configuration.

Uses moonshotai/kimi-k2.5 for comprehensive site analysis:
- Multi-step analysis of URL patterns
- Category discovery from sitemap and navigation
- Validation of generated patterns

Usage:
    python add_site.py https://thewoksoflife.com
    python add_site.py https://example.com --dry-run
    python add_site.py https://example.com --model gpt-4o  # Override model
"""

import sys
import os
import re
import json
import argparse
import asyncio
import requests
from pathlib import Path
from urllib.parse import urlparse, urljoin
from xml.etree import ElementTree as ET
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Tuple

from batch_llm_processor import strip_markdown_json
from tools.logging_utils import get_logger
from prompts import (
    SITE_ANALYSIS_SYSTEM_PROMPT,
    SITE_URL_ANALYSIS_PROMPT,
    SITE_CATEGORY_ANALYSIS_PROMPT,
    SITE_VALIDATION_PROMPT,
)

logger = get_logger(__name__)

# Site analysis model - DeepSeek V3.2 has 163K context and controllable reasoning
# Note: We disable reasoning for site analysis (faster, cheaper, avoids token exhaustion)
SITE_ANALYSIS_MODEL = "deepseek/deepseek-v3.2"

# Headers for web requests
REQUEST_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

SCRAPER_TEMPLATE = '''"""{site_name} scraper.

Generated by add_site.py with comprehensive LLM analysis.
{site_description}
"""

import re
from recipe_urls._abstract import AbstractScraper


class {class_name}Scraper(AbstractScraper):
    RECIPE_PATTERN = re.compile(r"{recipe_pattern}")
    UNWANTED_PATTERNS = [
        re.compile(pattern)
        for pattern in {unwanted_patterns}
    ]
    {category_pages_section}
    @classmethod
    def host(cls):
        return "{hostname}"
'''

# Prompts are now imported from prompts.py:
# - SITE_URL_ANALYSIS_PROMPT
# - SITE_CATEGORY_ANALYSIS_PROMPT  
# - SITE_VALIDATION_PROMPT


async def call_site_analysis_llm(prompt: str, model: str = None) -> str:
    """
    Call LLM for site analysis using the powerful analysis model.
    
    Args:
        prompt: Analysis prompt
        model: Override model (default: SITE_ANALYSIS_MODEL)
        
    Returns:
        LLM response string
    """
    from config import OPENROUTER_API_KEY, CHAT_API_URL, CHAT_MODEL
    import aiohttp
    
    model = model or SITE_ANALYSIS_MODEL
    
    if not OPENROUTER_API_KEY:
        raise RuntimeError("OPENROUTER_API_KEY not configured. Set via Settings ‚Üí API Credentials or data/secrets.yaml")
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "HTTP-Referer": "https://github.com/deepseekcoder2/ayechef",
        "X-Title": "Aye Chef Site Analyzer"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": SITE_ANALYSIS_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.1,
        "max_tokens": 4000,
        "reasoning": {"enabled": False}  # Disable reasoning for faster/cheaper responses
    }
    
    async with aiohttp.ClientSession() as session:
        async with session.post(
            f"{CHAT_API_URL}/chat/completions",
            json=payload,
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=120)
        ) as response:
            if response.status != 200:
                error_text = await response.text()
                # If the model isn't available, try falling back to default
                if response.status in (400, 404) and model == SITE_ANALYSIS_MODEL:
                    print(f"  WARNING: Model {model} not available, falling back to {CHAT_MODEL}")
                    payload["model"] = CHAT_MODEL
                    async with session.post(
                        f"{CHAT_API_URL}/chat/completions",
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=120)
                    ) as retry_response:
                        if retry_response.status != 200:
                            retry_error = await retry_response.text()
                            raise RuntimeError(f"LLM API error {retry_response.status}: {retry_error}")
                        data = await retry_response.json()
                        return data["choices"][0]["message"]["content"].strip()
                raise RuntimeError(f"LLM API error {response.status}: {error_text}")
            
            data = await response.json()
            content = data["choices"][0]["message"]["content"]
            return content.strip()


def fetch_homepage_urls(url: str, max_urls: int = 100) -> List[str]:
    """Fetch URLs from the website homepage."""
    print(f"  Fetching URLs from homepage...")
    
    try:
        response = requests.get(url, headers=REQUEST_HEADERS, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        
        parsed_base = urlparse(url)
        base_domain = parsed_base.netloc.lower()
        
        links = []
        for a in soup.find_all("a", href=True):
            href = a["href"]
            # Make absolute URL
            if href.startswith("/"):
                href = f"{parsed_base.scheme}://{parsed_base.netloc}{href}"
            elif not href.startswith("http"):
                continue
            
            # Filter to same domain
            if urlparse(href).netloc.lower() == base_domain:
                links.append(href)
        
        unique_links = list(set(links))[:max_urls]
        print(f"    Found {len(unique_links)} URLs on homepage")
        return unique_links
        
    except Exception as e:
        print(f"    Error fetching homepage: {e}")
        return []


def fetch_sitemap_urls(url: str, max_urls: int = 500) -> Tuple[List[str], List[str]]:
    """
    Fetch URLs from sitemap, separating recipe URLs from category URLs.
    
    Returns:
        Tuple of (all_urls, category_urls)
    """
    print(f"  Fetching sitemap...")
    
    parsed = urlparse(url)
    base_domain = f"{parsed.scheme}://{parsed.netloc}"
    
    sitemap_locations = [
        f"{base_domain}/sitemap.xml",
        f"{base_domain}/sitemap_index.xml",
        f"{base_domain}/sitemap-index.xml",
    ]
    
    all_urls = []
    category_urls = []
    
    def parse_sitemap(sitemap_url: str, depth: int = 0) -> None:
        if depth > 3:  # Prevent infinite recursion
            return
        
        try:
            response = requests.get(sitemap_url, headers=REQUEST_HEADERS, timeout=30)
            if response.status_code != 200:
                return
            
            root = ET.fromstring(response.content)
            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            
            # Check for sitemap index
            sitemap_refs = root.findall('.//sm:sitemap/sm:loc', ns)
            if sitemap_refs:
                print(f"    Found sitemap index with {len(sitemap_refs)} sitemaps")
                for ref in sitemap_refs:
                    sub_url = ref.text
                    # Parse sub-sitemaps, including category sitemaps
                    parse_sitemap(sub_url, depth + 1)
                return
            
            # Regular sitemap - extract URLs
            loc_elements = root.findall('.//sm:url/sm:loc', ns)
            for loc in loc_elements:
                url_text = loc.text
                if url_text:
                    all_urls.append(url_text)
                    # Detect category URLs - expanded patterns based on real site research
                    url_lower = url_text.lower()
                    category_patterns = [
                        '/category/', '/categories/',
                        '/collection/', '/collections/',
                        '/tag/', '/tags/',
                        '/cuisine/', '/cuisines/',
                        '/recipes/',  # Many sites use /recipes/category-name/
                        '/rezepte/',  # German
                        '/recetas/',  # Spanish
                        '/ricette/',  # Italian
                    ]
                    if any(pattern in url_lower for pattern in category_patterns):
                        category_urls.append(url_text)
                        
        except Exception as e:
            logger.debug(f"    Error parsing sitemap {sitemap_url}: {e}")
    
    for sitemap_url in sitemap_locations:
        try:
            response = requests.head(sitemap_url, headers=REQUEST_HEADERS, timeout=10)
            if response.status_code == 200:
                parse_sitemap(sitemap_url)
                break
        except:
            continue
    
    print(f"    Found {len(all_urls)} URLs in sitemap ({len(category_urls)} category URLs)")
    return all_urls[:max_urls], category_urls


def validate_category_url(category_url: str, recipe_pattern: re.Pattern = None) -> dict:
    """
    Validate that a category URL actually exists and contains recipe links.
    
    Returns:
        dict with 'valid' bool and 'recipe_count' int
    """
    try:
        response = requests.get(category_url, headers=REQUEST_HEADERS, timeout=15)
        if response.status_code >= 400:
            return {'valid': False, 'reason': f'HTTP {response.status_code}', 'recipe_count': 0}
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Count links that look like recipe URLs
        # Look for links with slugified names (contain hyphens, reasonable length)
        recipe_like_links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            # Skip obvious non-recipe links
            if any(x in href.lower() for x in ['/category/', '/tag/', '/author/', '/page/', 
                                                 '/about', '/contact', '/privacy', '/search',
                                                 'javascript:', 'mailto:', '#']):
                continue
            # Check if it looks like a recipe URL (has slug-like structure)
            path = urlparse(href).path.rstrip('/')
            if path:
                slug = path.split('/')[-1]
                # Recipe slugs typically: have hyphens, 10-80 chars, no file extensions
                if (slug and '-' in slug and 10 <= len(slug) <= 80 
                    and '.' not in slug and not slug.isdigit()):
                    recipe_like_links.append(href)
        
        # Dedupe
        unique_recipes = len(set(recipe_like_links))
        
        # Need at least 3 recipe-like links to be considered valid
        is_valid = unique_recipes >= 3
        
        return {
            'valid': is_valid,
            'recipe_count': unique_recipes,
            'reason': 'OK' if is_valid else f'Only {unique_recipes} recipe links found'
        }
        
    except requests.exceptions.Timeout:
        return {'valid': False, 'reason': 'Timeout', 'recipe_count': 0}
    except Exception as e:
        return {'valid': False, 'reason': str(e)[:50], 'recipe_count': 0}


def validate_categories(base_url: str, category_pages: dict) -> dict:
    """
    Validate all discovered categories by checking if URLs exist and contain recipes.
    
    Returns:
        dict with 'validated' (valid categories) and 'rejected' (invalid ones)
    """
    parsed = urlparse(base_url)
    base_domain = f"{parsed.scheme}://{parsed.netloc}"
    
    validated = {}
    rejected = {}
    
    print(f"  Validating {len(category_pages)} categories...")
    
    for name, path in category_pages.items():
        # Build full URL
        if path.startswith('http'):
            full_url = path
        else:
            full_url = base_domain + (path if path.startswith('/') else '/' + path)
        
        result = validate_category_url(full_url)
        
        if result['valid']:
            validated[name] = path
            print(f"    ‚úì {name}: {result['recipe_count']} recipes")
        else:
            rejected[name] = {'path': path, 'reason': result['reason']}
            print(f"    ‚úó {name}: {result['reason']}")
    
    return {'validated': validated, 'rejected': rejected}


def fetch_navigation_structure(url: str) -> str:
    """Extract navigation menu structure from the site, focusing on category-like links."""
    print(f"  Analyzing navigation structure...")
    
    try:
        response = requests.get(url, headers=REQUEST_HEADERS, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        
        parsed_base = urlparse(url)
        
        nav_items = []
        seen_hrefs = set()
        
        # Patterns that suggest category pages
        category_indicators = [
            '/category/', '/categories/', '/collection/', '/collections/',
            '/recipes/', '/tag/', '/tags/', '/cuisine/', '/cuisines/',
            '/rezepte/', '/recetas/', '/ricette/'
        ]
        
        # Look for navigation elements
        nav_elements = soup.find_all(['nav', 'header'])
        for nav in nav_elements:
            for a in nav.find_all('a', href=True):
                text = a.get_text(strip=True)
                href = a['href']
                
                # Normalize href
                if href.startswith('/'):
                    full_href = f"{parsed_base.scheme}://{parsed_base.netloc}{href}"
                elif not href.startswith('http'):
                    continue
                else:
                    full_href = href
                
                # Skip if already seen or not useful
                if full_href in seen_hrefs:
                    continue
                if not text or len(text) > 50 or len(text) < 2:
                    continue
                # Skip obvious non-category links
                if any(x in href.lower() for x in ['javascript:', 'mailto:', '#', 
                                                     '/cart', '/login', '/account',
                                                     '/search', '.jpg', '.png']):
                    continue
                
                seen_hrefs.add(full_href)
                
                # Prioritize links that look like categories
                is_category_like = any(ind in href.lower() for ind in category_indicators)
                prefix = "[LIKELY CATEGORY] " if is_category_like else ""
                nav_items.append(f"{prefix}{text}: {href}")
        
        # Also look for menu classes
        menu_elements = soup.find_all(class_=re.compile(r'menu|nav', re.I))
        for menu in menu_elements[:5]:
            for a in menu.find_all('a', href=True):
                text = a.get_text(strip=True)
                href = a['href']
                
                if href.startswith('/'):
                    full_href = f"{parsed_base.scheme}://{parsed_base.netloc}{href}"
                elif not href.startswith('http'):
                    continue
                else:
                    full_href = href
                
                if full_href in seen_hrefs:
                    continue
                if not text or len(text) > 50 or len(text) < 2:
                    continue
                    
                seen_hrefs.add(full_href)
                is_category_like = any(ind in href.lower() for ind in category_indicators)
                prefix = "[LIKELY CATEGORY] " if is_category_like else ""
                item = f"{prefix}{text}: {href}"
                if item not in nav_items:
                    nav_items.append(item)
        
        # Sort so category-like items appear first
        nav_items.sort(key=lambda x: (0 if x.startswith('[LIKELY CATEGORY]') else 1, x))
        
        result = "\n".join(nav_items[:50]) if nav_items else "No navigation links found"
        category_count = sum(1 for item in nav_items if '[LIKELY CATEGORY]' in item)
        print(f"    Found {len(nav_items)} navigation items ({category_count} likely categories)")
        return result
        
    except Exception as e:
        print(f"    Error analyzing navigation: {e}")
        return "Could not analyze navigation"


async def analyze_site_comprehensive(url: str, model: str = None) -> Optional[Dict]:
    """
    Perform comprehensive multi-step site analysis.
    
    Args:
        url: Website URL
        model: Optional model override
        
    Returns:
        Complete analysis dict with patterns and categories
    """
    print(f"\n{'=' * 60}")
    print(f"COMPREHENSIVE SITE ANALYSIS")
    print(f"Model: {model or SITE_ANALYSIS_MODEL}")
    print(f"{'=' * 60}\n")
    
    parsed = urlparse(url)
    hostname = parsed.netloc.lower().replace('www.', '')
    
    # Step 0: Gather data
    print("Step 0: Gathering site data...")
    homepage_urls = fetch_homepage_urls(url)
    sitemap_urls, category_urls = fetch_sitemap_urls(url)
    nav_structure = fetch_navigation_structure(url)
    
    if not homepage_urls and not sitemap_urls:
        print("ERROR: Could not fetch any URLs from the site")
        return None
    
    # Step 1: URL Pattern Analysis
    print("\nStep 1: Analyzing URL patterns...")
    step1_prompt = SITE_URL_ANALYSIS_PROMPT.format(
        url=url,
        hostname=hostname,
        homepage_urls="\n".join(f"- {u}" for u in homepage_urls[:50]),
        sitemap_urls="\n".join(f"- {u}" for u in sitemap_urls[:100]) if sitemap_urls else "No sitemap found"
    )
    
    try:
        step1_response = await call_site_analysis_llm(step1_prompt, model)
        step1_result = json.loads(strip_markdown_json(step1_response))
        print(f"  Recipe pattern: {step1_result.get('recipe_pattern')}")
        print(f"  URL structure: {step1_result.get('url_structure')}")
        print(f"  Confidence: {step1_result.get('confidence')}")
    except Exception as e:
        print(f"  ERROR in Step 1: {e}")
        return None
    
    # Step 2: Category Analysis
    print("\nStep 2: Analyzing category structure...")
    step2_prompt = SITE_CATEGORY_ANALYSIS_PROMPT.format(
        url=url,
        hostname=hostname,
        category_urls="\n".join(f"- {u}" for u in category_urls[:50]) if category_urls else "No category URLs found in sitemap",
        nav_structure=nav_structure,
        url_structure=step1_result.get('url_structure', 'unknown')
    )
    
    try:
        step2_response = await call_site_analysis_llm(step2_prompt, model)
        step2_result = json.loads(strip_markdown_json(step2_response))
        has_categories = step2_result.get('has_categories', False)
        category_pages = step2_result.get('category_pages', {})
        print(f"  LLM found categories: {has_categories}")
        if has_categories:
            print(f"  LLM proposed {len(category_pages)} categories")
            if step2_result.get('evidence'):
                print(f"  Evidence provided: {len(step2_result.get('evidence', {}))} items")
    except Exception as e:
        print(f"  ERROR in Step 2: {e}")
        step2_result = {'has_categories': False, 'category_pages': {}}
    
    # Step 2.5: VALIDATE categories by actually fetching them
    if step2_result.get('category_pages'):
        print("\nStep 2.5: Validating discovered categories (HTTP check)...")
        validation_result = validate_categories(url, step2_result['category_pages'])
        
        # Replace with only validated categories
        step2_result['category_pages'] = validation_result['validated']
        step2_result['rejected_categories'] = validation_result['rejected']
        step2_result['has_categories'] = bool(validation_result['validated'])
        
        print(f"  Validated: {len(validation_result['validated'])} categories")
        if validation_result['rejected']:
            print(f"  Rejected: {len(validation_result['rejected'])} categories (invalid URLs)")
    
    # Step 3: Validation
    print("\nStep 3: Validating configuration...")
    
    # Prepare test URLs
    sample_recipe_urls = [u for u in (homepage_urls + sitemap_urls)[:20] 
                         if not any(x in u.lower() for x in ['/category/', '/tag/', '/author/', '/about/'])][:10]
    sample_non_recipe_urls = category_urls[:5] + [u for u in homepage_urls 
                                                   if any(x in u.lower() for x in ['/about/', '/contact/', '/author/'])][:5]
    
    step3_prompt = SITE_VALIDATION_PROMPT.format(
        url=url,
        recipe_pattern=step1_result.get('recipe_pattern'),
        unwanted_patterns=step1_result.get('unwanted_patterns'),
        category_pages=step2_result.get('category_pages', {}),
        sample_recipe_urls="\n".join(f"- {u}" for u in sample_recipe_urls) or "No sample recipe URLs",
        sample_non_recipe_urls="\n".join(f"- {u}" for u in sample_non_recipe_urls) or "No sample non-recipe URLs"
    )
    
    try:
        step3_response = await call_site_analysis_llm(step3_prompt, model)
        step3_result = json.loads(strip_markdown_json(step3_response))
        print(f"  Pattern valid: {step3_result.get('pattern_valid')}")
        print(f"  Unwanted valid: {step3_result.get('unwanted_valid')}")
        print(f"  Categories valid: {step3_result.get('categories_valid')}")
        
        if step3_result.get('issues'):
            print(f"  Issues: {step3_result.get('issues')}")
    except Exception as e:
        print(f"  WARNING in Step 3: {e}")
        step3_result = {}
    
    # Compile final result
    final_result = {
        'recipe_pattern': step3_result.get('final_recipe_pattern', step1_result.get('recipe_pattern')),
        'unwanted_patterns': step3_result.get('final_unwanted_patterns', step1_result.get('unwanted_patterns', [])),
        'category_pages': step3_result.get('final_category_pages', step2_result.get('category_pages', {})),
        'has_categories': bool(step2_result.get('has_categories')) and bool(step2_result.get('category_pages')),
        'url_structure': step1_result.get('url_structure'),
        'confidence': step1_result.get('confidence'),
        'explanation': step1_result.get('explanation', ''),
        'validation_issues': step3_result.get('issues', []),
        'suggestions': step3_result.get('suggestions', [])
    }
    
    return final_result


def generate_scraper_code(url: str, analysis: Dict) -> Tuple[str, str, str]:
    """Generate scraper Python code with category support."""
    parsed = urlparse(url)
    hostname = parsed.netloc.lower().replace('www.', '')
    
    # Generate class name from hostname
    domain_parts = hostname.split('.')[0]
    class_name = ''.join(word.capitalize() for word in domain_parts.split('-'))
    
    # Site name for comment
    site_name = hostname.split('.')[0].replace('-', ' ').title()
    
    # Site description
    url_structure = analysis.get('url_structure', 'unknown')
    site_description = f"URL structure: {url_structure}"
    if analysis.get('explanation'):
        site_description += f"\n{analysis['explanation']}"
    
    # Format unwanted patterns
    unwanted = analysis.get('unwanted_patterns', [])
    unwanted_str = repr(unwanted)
    
    # Format category pages section
    category_pages = analysis.get('category_pages', {})
    if category_pages and analysis.get('has_categories'):
        # Format as multi-line dict
        category_lines = []
        for name, path in sorted(category_pages.items()):
            category_lines.append(f"        '{name}': '{path}',")
        category_pages_section = f"""
    # Category pages for bulk import with category selection
    CATEGORY_PAGES = {{
{chr(10).join(category_lines)}
    }}
"""
    else:
        category_pages_section = ""
    
    code = SCRAPER_TEMPLATE.format(
        site_name=site_name,
        site_description=site_description,
        class_name=class_name,
        recipe_pattern=analysis['recipe_pattern'],
        unwanted_patterns=unwanted_str,
        category_pages_section=category_pages_section,
        hostname=hostname
    )
    
    return code, class_name, hostname


def main():
    parser = argparse.ArgumentParser(
        description="Add support for a new recipe website with comprehensive LLM analysis",
        epilog="""
Examples:
  python add_site.py https://thewoksoflife.com
  python add_site.py https://example.com --dry-run
  python add_site.py https://example.com --model gpt-4o
  python add_site.py https://example.com --yes  # Auto-confirm (for automation)
        """
    )
    parser.add_argument("url", help="Website URL to add support for")
    parser.add_argument("--dry-run", action="store_true", help="Show generated code without saving")
    parser.add_argument("--model", help=f"Override LLM model (default: {SITE_ANALYSIS_MODEL})")
    parser.add_argument("--yes", "-y", action="store_true", help="Auto-confirm without prompting")
    
    args = parser.parse_args()

    # Normalize URL
    parsed = urlparse(args.url)
    if not parsed.scheme:
        args.url = f"https://{args.url}"
        parsed = urlparse(args.url)
    
    hostname = parsed.netloc.lower().replace('www.', '')
    module_name = hostname.split('.')[0].replace('-', '')
    
    # User-generated scrapers go in data/scrapers/ (writable in Docker)
    from config import DATA_DIR
    data_scrapers_dir = DATA_DIR / "scrapers"
    data_scrapers_dir.mkdir(parents=True, exist_ok=True)
    
    # Check both locations for existing scraper
    builtin_path = Path(f"recipe_urls/sites/{module_name}.py")
    user_path = data_scrapers_dir / f"{module_name}.py"
    
    if builtin_path.exists():
        print(f"\nBuilt-in scraper already exists: {builtin_path}")
        print("This site is already supported.")
        sys.exit(1)
    
    if user_path.exists():
        print(f"\nUser scraper already exists: {user_path}")
        print("Delete it first if you want to regenerate.")
        sys.exit(1)
    
    scraper_path = user_path

    print(f"\n{'=' * 60}")
    print(f"Adding support for: {hostname}")
    print(f"Using model: {args.model or SITE_ANALYSIS_MODEL}")
    print(f"{'=' * 60}")

    # Run comprehensive analysis
    analysis = asyncio.run(analyze_site_comprehensive(args.url, args.model))
    if not analysis:
        print("\nAnalysis failed. You may need to create the scraper manually.")
        sys.exit(1)

    # Print analysis summary
    print(f"\n{'=' * 60}")
    print("ANALYSIS SUMMARY")
    print(f"{'=' * 60}")
    print(f"Recipe pattern: {analysis.get('recipe_pattern')}")
    print(f"Unwanted patterns: {len(analysis.get('unwanted_patterns', []))} patterns")
    print(f"Has categories: {analysis.get('has_categories')}")
    if analysis.get('has_categories'):
        print(f"Categories found: {len(analysis.get('category_pages', {}))}")
        for name in list(analysis.get('category_pages', {}).keys())[:10]:
            print(f"  - {name}")
        if len(analysis.get('category_pages', {})) > 10:
            print(f"  ... and {len(analysis.get('category_pages', {})) - 10} more")
    print(f"Confidence: {analysis.get('confidence')}")
    
    if analysis.get('validation_issues'):
        print(f"\nValidation issues:")
        for issue in analysis['validation_issues']:
            print(f"  ‚ö†Ô∏è  {issue}")
    
    if analysis.get('suggestions'):
        print(f"\nSuggestions:")
        for suggestion in analysis['suggestions']:
            print(f"  üí° {suggestion}")

    # Generate code
    code, class_name, hostname = generate_scraper_code(args.url, analysis)

    print(f"\n{'=' * 60}")
    print("GENERATED SCRAPER CODE")
    print(f"{'=' * 60}")
    print(code)

    if args.dry_run:
        print(f"\n[DRY RUN] Would save to: {scraper_path}")
        return

    # Confirm (skip if --yes flag)
    if not args.yes:
        response = input(f"\nSave this scraper? (y/n): ").lower().strip()
        if response not in ['y', 'yes']:
            print("Cancelled.")
            return

    # Save scraper file to data/scrapers/
    with open(scraper_path, 'w') as f:
        f.write(code)
    print(f"\n‚úÖ Saved: {scraper_path}")

    print(f"\n{'=' * 60}")
    print("SUCCESS")
    print(f"{'=' * 60}")
    print(f"Site added: {hostname}")
    print(f"\nTest it:")
    print(f"  python import_site.py {args.url} --dry-run")
    if analysis.get('has_categories'):
        print(f"\nThis site has {len(analysis.get('category_pages', {}))} categories configured.")
        print("Users can select categories when bulk importing.")


if __name__ == "__main__":
    main()
